{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Running FairNow's User Data Bias Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "#### FairNow's User Data Testing is a way to evaluate a model for bias using real data. Please do not send any PII data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Prerequisites:\n",
    "\n",
    "#### To use this notebook, you'll need a `Client ID` and `Client Secret`. These will either have been provided to you, or you can generate from https://app.fairnow.ai and going the the Admin menu. This notebook assumes you have these available to enter when prompted.\n",
    "\n",
    "#### To run the simulation you will need an `application_id` and `application_version` for the specific model you want to test. Details of how to create and lookup AI Applications can be found in the `Applications AI` notebook.\n",
    "\n",
    "#### Finally, you'll need a `threshold` value, which is the value at which anyone with a score above is considered a passing score.\n",
    "\n",
    "#### Running the following cell will prompt you for the `Client ID` and `Client Secret` and create a `client` instance that can be used to communicate with the Fairnow APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from time import sleep\n",
    "from getpass import getpass\n",
    "import httpx\n",
    "from httpx_auth import OAuth2, OAuth2ClientCredentials\n",
    "\n",
    "client_id = \"{client_id}\" # Replace with your Client Id\n",
    "client_secret = getpass(\"Client Secret\")\n",
    "fairnow_token_endpoint = \"https://auth.fairnow.ai/oauth2/token\"\n",
    "\n",
    "auth = OAuth2ClientCredentials(\n",
    "    token_url=fairnow_token_endpoint,\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    ")\n",
    "\n",
    "fairnow_base_url = \"https://api.fairnow.ai/v2\"\n",
    "client = httpx.Client(base_url=fairnow_base_url, auth=auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "#### You will also need to prepare a CSV file to upload containing data. The first row contains the column names.\n",
    "\n",
    "#### The following columns are required:\n",
    "* `TimeStamp` (ISO8601 Timestamp, e.g `2023-12-14T16:26:05.898156Z`)\n",
    "* `Score` (a number between 0 and 1)\n",
    "*  `Each of the  Protected Class Columns` (see `Generating User Bias Test Data` notebook for example on how to lookup the column names and example for generating a CSV to upload.)\n",
    "\n",
    "#### Additional columns can be added to allow filtering of data, e.g. `Job Title`, `Location` etc\n",
    "\n",
    "### Place the CSV file in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_file_name = 'scores.csv'  # Change the filename if different"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### Next we create a test. The test needs to the `application_id` and `application_version`, along with a threshold value. The response will include the `test_id` and a pre-signed URL used upload the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_test_route = \"/tests/start/\"\n",
    "\n",
    "application_id = None               # Replace with your own Application Id\n",
    "application_version = \"1.0\"         #  Change if you are working with a different version \n",
    "threshold = 0.5                     # Change to your own threshold setting\n",
    "\n",
    "if threshold <= 0.0 or threshold >= 1.0:\n",
    "    raise ValueError(\"Threshold must be between 0.0 and 1.0\")\n",
    "\n",
    "request_body = {\n",
    "    \"application_id\": application_id,\n",
    "    \"application_version\": application_version,\n",
    "    \"test_name\": \"API Client Testing\",\n",
    "    \"test_description\": \"Testing User Bias\",\n",
    "    \"test_type\": \"fairness_ml_user\",\n",
    "    \"threshold\": threshold\n",
    "}\n",
    "\n",
    "response = client.post(start_test_route, json=request_body, timeout=None)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"New Test has been created:\")\n",
    "    print(json.dumps(response.json(), indent=4))\n",
    "else:\n",
    "    print(f\"API Error Response: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### Use the pre-signed upload url to upload the scores file. Once the scores are uploaded, this triggers the analysis job. This runs in the background again and can take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_body = response.json()\n",
    "test_id = response_body[\"test_id\"]\n",
    "presigned_upload = response_body[\"presigned_url_scores_upload\"]\n",
    "upload_url = presigned_upload[\"url\"]\n",
    "upload_fields = presigned_upload[\"fields\"]\n",
    "upload_key = upload_fields[\"key\"]\n",
    "\n",
    "data_file = {'file': (upload_key, open(scores_file_name, 'rb'))}\n",
    "\n",
    "# Upload the scores CSV file\n",
    "response = httpx.post(upload_url, data=upload_fields, files=data_file)\n",
    "\n",
    "if response.status_code == 204:\n",
    "    print(\"File has been uploaded.\")\n",
    "else:\n",
    "    print(f\"Error uploading file: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### We'll query the API again to know when the analysis has been finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_route = \"/tests/\"\n",
    "query_parameters = {\n",
    "    \"test_id\": test_id,\n",
    "    \"application_id:\": application_id,\n",
    "    \"application_version:\": application_version,\n",
    "}\n",
    "\n",
    "response = client.get(test_route, params=query_parameters)\n",
    "if response.status_code == 200:\n",
    "    current_status = response.json()[\"status\"][\"id\"]\n",
    "    print(f\"Current test status: {current_status}\")\n",
    "else:\n",
    "    raise ValueError(f\"API Error Response: {response.status_code} - {response.text}\")\n",
    "\n",
    "while current_status not in ['ready', 'error']:\n",
    "    sleep(15)\n",
    "    response = client.get(test_route, params=query_parameters)\n",
    "    if response.status_code == 200:\n",
    "        current_status = response.json()[\"status\"][\"id\"]\n",
    "        print(f\"Current test status: {current_status}\")\n",
    "    else:\n",
    "        raise ValueError(f\"API Error Response: {response.status_code} - {response.text}\")\n",
    "\n",
    "if current_status == 'error':\n",
    "    error_details = response.json()[\"file_validation_report\"]\n",
    "    print('Analysis encountered an error. Details:')\n",
    "    print(error_details)\n",
    "else:\n",
    "    print(f'Analysis results ready to download.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Now the the testing is complete you can check the result in the Fairnow application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_url = response.json()[\"results_url\"]\n",
    "print(f\"Check your results in the Fairnow application at '{test_results_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Optionally, you can download the raw analysis results with presigned link. The output is a csv file with the results of bias by protected classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "presigned_test_results_url = response.json()[\"presigned_url_test_results\"]\n",
    "\n",
    "response = httpx.get(presigned_test_results_url)\n",
    "\n",
    "with open('results.csv', 'wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### Now read the results from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat results.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
